version: "3.3"
services:
  grafana:
    image: grafana/grafana
    restart: always
    user: "1000"
    networks:
      - system_default
      - services-monitoring_default
    secrets:
      - authentik_jwk.json
    volumes:
      - grafana_data:/var/lib/grafana/
      - ./grafana/config/grafana.ini:/etc/grafana/grafana.ini
    environment:
      GF_AUTH_GENERIC_OAUTH_ENABLED: "true"
      GF_AUTH_GENERIC_OAUTH_NAME: "authentik"
      GF_AUTH_GENERIC_OAUTH_CLIENT_ID: ${GF_AUTH_GENERIC_OAUTH_CLIENT_ID}
      GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET: ${GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET}
      GF_AUTH_GENERIC_OAUTH_SCOPES: "openid profile email"
      GF_AUTH_GENERIC_OAUTH_AUTH_URL: "https://auth.apps.hskrk.pl/application/o/authorize/"
      GF_AUTH_GENERIC_OAUTH_TOKEN_URL: "http://authentik:9000/application/o/token/"
      GF_AUTH_GENERIC_OAUTH_API_URL: "https://authentik:9000/application/o/userinfo/"
      GF_AUTH_SIGNOUT_REDIRECT_URL: "https://auth.apps.hskrk.pl/application/o/grafana/end-session/"
      # Optionally enable auto-login (bypasses Grafana login screen)
      GF_AUTH_OAUTH_AUTO_LOGIN: "true"
      # Optionally map user groups to Grafana roles
      GF_AUTH_GENERIC_OAUTH_ROLE_ATTRIBUTE_PATH: "contains(groups[*], 'noc') && 'Admin' || contains(groups[*], 'staff') && 'Editor' || 'Viewer'"
      GF_AUTH_GENERIC_OAUTH_GROUPS_ATTRIBUTE_PATH: "groups"
      GF_RENDERING_SERVER_URL: http://renderer:8081/render
      GF_RENDERING_CALLBACK_URL: http://grafana:3000/
      GF_INSTALL_PLUGINS: "grafana-clock-panel, grafana-simple-json-datasource"
    deploy:
      labels:
        - "traefik.enable=true"
        - "traefik.docker.network=system_default"
        - "traefik.http.services.grafana.loadbalancer.server.port=3000"
        - "traefik.http.routers.grafana.rule=Host(`grafana.apps.hskrk.pl`)"
        - "traefik.http.routers.grafana.tls=true"
        - "traefik.http.routers.grafana.tls.certresolver=myresolver"

  renderer:
    image: grafana/grafana-image-renderer:latest
    networks:
      - services-monitoring_default
    environment:
      - HTTP_HOST=0.0.0.0
      - RENDERING_MODE=reusable

  loki:
    image: grafana/loki:2.6.1
    networks:
      - services-monitoring_default
    restart: always
    user: "0"
    command: -config.file=/etc/loki/local-config.yaml
    volumes:
      - ./loki/config/config.yml:/etc/loki/local-config.yaml
      - loki_data:/data/
  prometheus:
    image: prom/prometheus:latest
    volumes:
      - ./prometheus/:/etc/prometheus/
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    networks:
      - services-monitoring_default
      - system_default
    restart: always
    deploy:
      labels:
        - "traefik.enable=true"
        - "traefik.docker.network=system_default"
        - "traefik.http.services.prometheus.loadbalancer.server.port=9090"
        - "traefik.http.routers.prometheus.rule=Host(`prometheus.apps.hskrk.pl`)"
        - "traefik.http.routers.prometheus.tls=true"
        - "traefik.http.routers.prometheus.tls.certresolver=myresolver"
        - "traefik.http.routers.prometheus.middlewares=authentik@docker"
  pushgateway:
    image: prom/pushgateway:latest
    restart: unless-stopped
    depends_on:
      - prometheus
    networks:
      - services-monitoring_default
      - system_default
    command:
      - '--log.level=info'
      - '--web.enable-admin-api'
      - '--web.enable-lifecycle'
    labels:
        - "traefik.enable=true"
        - "traefik.docker.network=system_default"
        - "traefik.http.services.pushgateway.loadbalancer.server.port=9091"
        - "traefik.http.routers.pushgateway.rule=Host(`pushgateway.apps.hskrk.pl`)"
        - "traefik.http.routers.pushgateway.tls=true"
        - "traefik.http.routers.pushgateway.tls.certresolver=myresolver"

  node-exporter:
    image: quay.io/prometheus/node-exporter:latest
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
      - /:/host:ro,rslave
    command:
      - '--path.rootfs=/host'
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - --collector.filesystem.ignored-mount-points
      - "^/(sys|proc|dev|host|etc|rootfs/var/lib/docker/containers|rootfs/var/lib/docker/overlay2|rootfs/run/docker/netns|rootfs/var/lib/docker/aufs)($$|/)"
    networks:
      - services-monitoring_default
    restart: always
    deploy:
      mode: global

  cadvisor:
    image: gcr.io/cadvisor/cadvisor
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:rw
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
    networks:
      - services-monitoring_default
    restart: always
    deploy:
      mode: global

  snmp-exporter:
    image: quay.io/prometheus/snmp-exporter
    restart: always
    networks:
      - services-monitoring_default
      - system_default
    deploy:
      labels:
        - "traefik.enable=true"
        - "traefik.docker.network=system_default"
        - "traefik.http.services.snmp-exporter.loadbalancer.server.port=9116"
        - "traefik.http.routers.snmp-exporter.rule=Host(`snmp-exporter.apps.hskrk.pl`)"
        - "traefik.http.routers.snmp-exporter.tls=true"
        - "traefik.http.routers.snmp-exporter.tls.certresolver=myresolver"
        - "traefik.http.routers.snmp-exporter.middlewares=authentik@docker"

  backblaze-monitoring:
    image: docker.io/molu8bits/s3bucket_exporter:0.3
    restart: always
    networks:
      - services-monitoring_default

    environment:
      S3_ENDPOINT: https://s3.us-east-005.backblazeb2.com
      LISTEN_PORT: ":9655"
      S3_ACCESS_KEY: ${B2_S3_ACCESS_KEY}
      S3_SECRET_KEY: ${B2_S3_SECRET_KEY}
      S3_NAME: hskrk-backups
      S3_REGION: us-east-1
      S3_DISABLE_SSL: "False"
      S3_DISABLE_ENDPOINT_HOST_PREFIX: "True"
      S3_FORCE_PATH_STYLE: "True"
      LOG_LEVEL: Info
    healthcheck:
      test: curl -s -f http://localhost:9655/metrics
      timeout: 30s
      interval: 2m
      retries: 3

  promtail:
    image: grafana/promtail:2.6.1
    networks:
      - services-monitoring_default
    restart: unless-stopped
    volumes:
      - /var/log:/var/log
      - ./promtail/config/config.yml:/etc/promtail/config.yml
    command: -config.file=/etc/promtail/config.yml
    ports:
      - "1514:1514"

  monocker:
    container_name: monocker
    image: petersem/monocker
    environment:
      # Optional label to preface messages. Handy if you are running multiple versions of Monocker
      SERVER_LABEL: 'apps'
      MESSAGE_PLATFORM: "telegram@${TELEGRAM_BOT_ID}@${TELEGRAM_CHAT_ID}"
      LABEL_ENABLE: 'false'
      # Optional - only show when container state changes to being offline (paused, exited, running (unhealthy), or dead) - default is false
      ONLY_OFFLINE_STATES: 'false'
      # [Optional] - Regardless of any other settings, you can ignore or include 'exited'
      EXCLUDE_EXITED: 'false'
      # [Optional] - Set the poll period in seconds. Defaults to 10 seconds, which is also the minimum.
      PERIOD: 10
      # [Optional] - Supress startup messages from being sent. Default is false
      DISABLE_STARTUP_MSG: 'false'
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    restart: unless-stopped

  healthchecks:
    container_name: healthchecks_pinger
    image: ghcr.io/curl/curl-container/curl:master
    environment:
      - HEALTHCHECKS_URL=${HEALTHCHECKS_URL}
    command: "-fsS ${HEALTHCHECKS_URL}"
    deploy:
      mode: replicated
      replicas: 0
      labels:
        - "swarm.cronjob.enable=true"
        - "swarm.cronjob.schedule=* * * * *"
        - "swarm.cronjob.skip-running=true"

secrets:
  authentik_key.pem:
    external: true
  authentik_jwk.json:
    external: true
  nut-upsd-password:
    external: true

volumes:
  grafana_data:
  influxdb_data:
  loki_data:
  prometheus_data:


networks:
  system_default:
    external: true
  services-monitoring_default:
    internal: true
